{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7561acd9",
      "metadata": {
        "id": "7561acd9"
      },
      "source": [
        "###  6.1. Sampling in Hidden Markov Models (Generate sample data)\n",
        "\n",
        "First, we will generate sample data (observations) by using the distribution of Hidden Markov Models (HMM).\n",
        "\n",
        "The distribution of the latent (hidden) variables $ \\{\\mathbf{z}_n\\} $ is discrete, and it then corresponds to a table of transitions.\n",
        "\n",
        "For sampling, we will create a set of latent (hidden) variables, $ \\{\\mathbf{z}_n\\} $, in which it has 3 states (i.e, $ K=3 $) with the following transition probabilities $ p(\\mathbf{z}_n|\\mathbf{z}_{n-1}) $.\n",
        "\n",
        "$$ A = \\begin{bmatrix} 0.7 & 0.15 & 0.15 \\\\ 0.0 & 0.5 & 0.5 \\\\ 0.3 & 0.35 & 0.35 \\end{bmatrix} $$\n",
        "\n",
        "From now, we will use the letter $ k \\in \\{0, 1, 2\\} $ for the corresponding 3 states, and  assume $ \\mathbf{z}_n = (z_{n,0}, z_{n,1}, z_{n,2}) $, in which $ z_{n,k^{\\prime}}=1 $ and $ z_{n,k \\neq k^{\\prime}}=0 $ in state $ k^{\\prime} $."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8c961a3",
      "metadata": {
        "id": "e8c961a3",
        "outputId": "0689aa81-16a7-452d-b2a7-32d6bf104889"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0, 0, 0, ..., 0, 0, 2])"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "np.random.seed(1000)  # For debugging and reproducibility\n",
        "\n",
        "N = 1000\n",
        "\n",
        "Z = np.array([0])\n",
        "for n in range(N):\n",
        "    prev_z = Z[len(Z) - 1]\n",
        "    if prev_z == 0:\n",
        "        post_z = np.random.choice(3, size=1, p=[0.7, 0.15, 0.15])\n",
        "    elif prev_z == 1:\n",
        "        post_z = np.random.choice(3, size=1, p=[0.0, 0.5, 0.5])\n",
        "    elif prev_z == 2:\n",
        "        post_z = np.random.choice(3, size=1, p=[0.3, 0.35, 0.35])\n",
        "    Z = np.append(Z, post_z)\n",
        "Z"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4751b087",
      "metadata": {
        "id": "4751b087"
      },
      "source": [
        "Next we will create the corresponding observation $ \\{\\mathbf{x}_n\\} $ for sampling.<br>\n",
        "Here we assume 2-dimensional **Gaussian distribution** $ \\mathcal{N}(\\mathbf{\\mu}_k, \\mathbf{\\Sigma}_k) $ for emission probabilities $ p(\\mathbf{x}_n|\\mathbf{z}_n) $, when $ \\mathbf{z}_n $ belongs to $ k $. ($ k=0,1,2 $)<br>\n",
        "In order to simplify, we also assume that parameters $ \\mathbf{\\mu}_k, \\mathbf{\\Sigma}_k $ are independent for different components $ k=0, 1, 2 $.\n",
        "\n",
        "In this example, we set $ \\mathbf{\\mu}_k, \\mathbf{\\Sigma}_k $ as follows.\n",
        "\n",
        "$$ \\mathbf{\\mu}_0=(16.0, 1.0), \\;\\; \\mathbf{\\Sigma}_0 = \\begin{bmatrix} 4.0 & 3.5 \\\\ 3.5 & 4.0 \\end{bmatrix} $$\n",
        "\n",
        "$$ \\mathbf{\\mu}_1=(1.0, 16.0), \\;\\; \\mathbf{\\Sigma}_1 = \\begin{bmatrix} 4.0 & 0.0 \\\\ 0.0 & 1.0 \\end{bmatrix} $$\n",
        "\n",
        "$$ \\mathbf{\\mu}_2=(-5.0, -5.0), \\;\\; \\mathbf{\\Sigma}_2 = \\begin{bmatrix} 1.0 & 0.0 \\\\ 0.0 & 4.0 \\end{bmatrix} $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d18dbe8",
      "metadata": {
        "id": "9d18dbe8",
        "outputId": "905cdbb2-a2ba-467e-dddd-9e4a2961d0e5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[16.10996367, -0.05478763],\n",
              "       [18.15392063,  3.77525205],\n",
              "       [16.73825958,  0.59324625],\n",
              "       ...,\n",
              "       [14.2188323 , -1.0984775 ],\n",
              "       [18.41063372,  5.28130838],\n",
              "       [-3.64054111, -4.00216984]])"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X = np.empty((0,2))\n",
        "for z_n in Z:\n",
        "    if z_n == 0:\n",
        "        x_n = np.random.multivariate_normal(\n",
        "            mean=[16.0, 1.0],\n",
        "            cov=[[4.0,3.5],[3.5,4.0]],\n",
        "            size=1)\n",
        "    elif z_n == 1:\n",
        "        x_n = np.random.multivariate_normal(\n",
        "            mean=[1.0, 16.0],\n",
        "            cov=[[4.0,0.0],[0.0,1.0]],\n",
        "            size=1)\n",
        "    elif z_n ==2:\n",
        "        x_n = np.random.multivariate_normal(\n",
        "            mean=[-5.0, -5.0],\n",
        "            cov=[[1.0,0.0],[0.0,4.0]],\n",
        "            size=1)\n",
        "    X = np.vstack((X, x_n))\n",
        "X"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6.2. EM algorithm in Hidden Markov Models (HMM)\n",
        "\n",
        "Now, using the given observation $ \\{ \\mathbf{x}_n \\} $, let's try to estimate the optimimal parameters in HMM.\n",
        "\n",
        "When we denote unknown parameters by $ \\mathbf{\\theta} $, our goal is to get the optimal parameters $ \\mathbf{\\theta} $ to maximize the following (1).\n",
        "\n",
        "$$ p(\\mathbf{X}|\\mathbf{\\theta}) = \\sum_{\\mathbf{Z}} p(\\mathbf{X},\\mathbf{Z}|\\mathbf{\\theta}) \\;\\;\\;\\;(1) $$\n",
        "\n",
        "where $ \\mathbf{Z} = \\{\\mathbf{z}_n\\} $ and $ \\mathbf{X} = \\{\\mathbf{x}_n\\} $\n",
        "\n",
        "In this example, we use the following parameters as $ \\mathbf{\\theta} = \\{ \\mathbf{\\pi}, \\mathbf{A}, \\mathbf{\\mu}, \\mathbf{\\Sigma} \\} $.\n",
        "\n",
        "- $ \\pi_k (k \\in \\{0, 1, 2\\}) $ : The possibility (scalar) for component $ k $ in initial latent node $ \\mathbf{z}_0 $. ($ \\Sigma_k \\pi_k = 1 $)\n",
        "- $ A_{j,k} \\; (j, k \\in \\{0, 1, 2\\}) $ : The transition probability (scalar) for the latent variable $ \\mathbf{z}_{n-1} $ to $ \\mathbf{z}_n $, in which $ \\mathbf{z}_{n-1} $ belongs to $ j $ and $ \\mathbf{z}_n $ belongs to $ k $. ($ \\Sigma_k A_{j,k} = 1 $)\n",
        "- $ \\mathbf{\\mu}_k $ : The mean (2-dimensional vector) for Gaussian distribution in emission probabilities $ p(\\mathbf{x}_n|\\mathbf{z}_n) $ when the latent variable $ \\mathbf{z}_n $ belongs to $ k $.\n",
        "- $ \\mathbf{\\Sigma}_k $ : The covariance matrix ($ 2 \\times 2 $ matrix) for Gaussian distribution in emission probabilities $ p(\\mathbf{x}_n|\\mathbf{z}_n) $ when the latent variable $ \\mathbf{z}_n $ belongs to $ k $.\n",
        "\n",
        "In (1), the number of parameters will rapidly increase, when the number of states $ K $ increases (in this example, $ K = 3 $). Furthermore it has summation (not multiplication) in distribution (1), and the log likelihood will then lead to complex expression in maximum likelihood estimation (MLE).<br>\n",
        "Therefore, it will be difficult to directly apply maximum likelihood estimation (MLE) for the expression (1).\n",
        "\n",
        "In practice, the expectationâ€“maximization algorithm (shortly, **EM algorithm**) can often be applied to solve parameters in HMM.<br>\n",
        "\n",
        "\n",
        "In EM algorithm for HMM, we start with initial parameters $ \\mathbf{\\theta}^{old} $, and optimize (find) new $ \\mathbf{\\theta} $ to maximize the following expression (2).<br>\n",
        "By repeating this operation, we can expect to reach to the likelihood parameters $ \\hat{\\mathbf{\\theta}} $.\n",
        "\n",
        "$$ Q(\\mathbf{\\theta}, \\mathbf{\\theta}^{old}) = \\sum_{\\mathbf{Z}} p(\\mathbf{Z}|\\mathbf{X}, \\mathbf{\\theta}^{old}) \\ln p(\\mathbf{X}, \\mathbf{Z}|\\mathbf{\\theta}) \\;\\;\\;\\;(2) $$\n",
        "\n",
        "> Note : For the essential idea of EM algorithm, see Chapter 9 in \"[Pattern Recognition and Machine Learning](https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf?ranMID=24542&ranEAID=TnL5HPStwNw&ranSiteID=TnL5HPStwNw-g4zE85KQgCXaCQfYBhtuFQ&epi=TnL5HPStwNw-g4zE85KQgCXaCQfYBhtuFQ&irgwc=1&OCID=AID2200057_aff_7593_1243925&tduid=%28ir__vhvv9m6caokf6nb62oprh029if2xo0rux3ga300300%29%287593%29%281243925%29%28TnL5HPStwNw-g4zE85KQgCXaCQfYBhtuFQ%29%28%29&irclickid=_vhvv9m6caokf6nb62oprh029if2xo0rux3ga300300)\" (Christopher M. Bishop, Microsoft)\n",
        "\n",
        "Now we denote the discrete probability $ p(\\mathbf{z}_n|\\mathbf{X},\\mathbf{\\theta}^{old}) $ by $ \\gamma(z_{n,k}) \\; (k=0,1,2) $, in which $ \\gamma(z_{n,k}) $ represents the probability of $ \\mathbf{z}_n $ for belonging to $ k $.<br>\n",
        "We also denote the discrete probability $ p(\\mathbf{z}_{n-1}, \\mathbf{z}_n | \\mathbf{X},\\mathbf{\\theta}^{old}) $ by $ \\xi(z_{n-1,j}, z_{n,k}) \\; (j,k=0,1,2) $, in which $ \\xi(z_{n-1,j}, z_{n,k}) $ represents the joint probability that $ \\mathbf{z}_{n-1} $ belongs to $ j $ and $ \\mathbf{z}_n $ belongs to $ k $.\n",
        "\n",
        "In Gaussian HMM (in the above model), the equation (2) is written as follows, using $ \\gamma() $ and $ \\xi() $.\n",
        "\n",
        "$$ Q(\\mathbf{\\theta}, \\mathbf{\\theta}^{old}) = \\sum_{k=0}^{K-1} \\gamma(z_{0,k}) \\ln{\\pi_k} + \\sum_{n=1}^{N-1} \\sum_{j=0}^{K-1} \\sum_{k=0}^{K-1} \\xi(z_{n-1,j},z_{n,k}) \\ln{A_{j,k}} + \\sum_{n=0}^{N-1} \\sum_{k=0}^{K-1} \\gamma(z_{n,k}) \\ln{p(\\mathbf{x}_n|\\mathbf{\\mu}_k, \\mathbf{\\Sigma}_k)} \\;\\;\\;\\;(3)$$\n",
        "\n",
        "where\n",
        "\n",
        "$$ \\gamma(\\mathbf{z}_n) = p(\\mathbf{z}_n|\\mathbf{X},\\mathbf{\\theta}^{old}) $$\n",
        "\n",
        "$$ \\xi(\\mathbf{z}_{n-1}, \\mathbf{z}_n) = p(\\mathbf{z}_{n-1}, \\mathbf{z}_n|\\mathbf{X},\\mathbf{\\theta}^{old}) $$\n",
        "\n",
        "It's known that $ \\gamma() $ and $ \\xi() $ can be given by the following $ \\alpha() $ and $ \\beta() $, which are determined recursively. (i.e, We can first determine all $ \\alpha() $ and $ \\beta() $ recursively, and then we can obtain $ \\gamma() $ and $ \\xi() $ with known $ \\alpha(), \\beta() $.)\n",
        "\n",
        "$$ \\gamma(z_{n,k}) = \\frac{\\alpha(z_{n,k})\\beta(z_{n,k})}{\\sum_{k=0}^{K-1} \\alpha(z_{n,k})\\beta(z_{n,k})} $$\n",
        "\n",
        "$$ \\xi(z_{n-1,j},z_{n,k}) = \\frac{\\alpha(z_{n-1,j})p(\\mathbf{x}_n|\\mathbf{\\mu}_k^{old}, \\mathbf{\\Sigma}_k^{old})A_{j,k}^{old}\\beta(z_{n,k})}{\\sum_{j=0}^{K-1} \\sum_{k=0}^{K-1} \\alpha(z_{n-1,j})p(\\mathbf{x}_n|\\mathbf{\\mu}_k^{old}, \\mathbf{\\Sigma}_k^{old})A_{j,k}^{old}\\beta(z_{n,k})} $$\n",
        "\n",
        "where all $ \\alpha() $ and $ \\beta() $ are recursively given by\n",
        "\n",
        "$$ \\alpha(z_{n,k}) = p(\\mathbf{x}_n|\\mathbf{\\mu}_k^{old}, \\mathbf{\\Sigma}_k^{old}) \\sum_{j=0}^{K-1} A_{jk}^{old} \\alpha(z_{n-1,j}) $$\n",
        "\n",
        "$$ \\beta(z_{n-1,k}) = \\sum_{j=0}^{K-1} A^{old}_{k,j} p(\\mathbf{x}_{n}|\\mathbf{\\mu}_j^{old}, \\mathbf{\\Sigma}_j^{old}) \\beta(z_{n,j}) $$\n",
        "\n",
        "Now we need the starting condition for recursion, $ \\alpha() $ and $ \\beta() $, and these are given as follows.\n",
        "\n",
        "$$ \\alpha(z_{0,k}) = \\pi_k^{old} p(\\mathbf{x}_0|\\mathbf{\\mu}_k^{old}, \\mathbf{\\Sigma}_k^{old}) $$\n",
        "\n",
        "$$ \\beta(z_{N-1,k}) = 1 $$\n",
        "\n",
        "> Note : You can check the proofs of these Gaussian HMM properties in Chapter 13 of \"[Pattern Recognition and Machine Learning](https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf?ranMID=24542&ranEAID=TnL5HPStwNw&ranSiteID=TnL5HPStwNw-g4zE85KQgCXaCQfYBhtuFQ&epi=TnL5HPStwNw-g4zE85KQgCXaCQfYBhtuFQ&irgwc=1&OCID=AID2200057_aff_7593_1243925&tduid=%28ir__vhvv9m6caokf6nb62oprh029if2xo0rux3ga300300%29%287593%29%281243925%29%28TnL5HPStwNw-g4zE85KQgCXaCQfYBhtuFQ%29%28%29&irclickid=_vhvv9m6caokf6nb62oprh029if2xo0rux3ga300300)\" (Christopher M. Bishop, Microsoft)\n",
        "\n",
        "Once you have got $ \\gamma() $ and $ \\xi() $, you can get the optimal $ \\mathbf{\\theta} = \\{ \\mathbf{\\pi}, \\mathbf{A}, \\mathbf{\\mu}, \\mathbf{\\Sigma} \\} $ to maximize (3) as follows by applying Lagrange multipliers.\n",
        "\n",
        "$$ \\pi_k = \\frac{\\gamma(z_{0,k})}{\\sum_{j=0}^{K-1} \\gamma(z_{0,j})} $$\n",
        "\n",
        "$$ A_{j,k} = \\frac{\\sum_{n=1}^{N-1} \\xi(z_{n-1,j},z_{n,k})}{\\sum_{l=0}^{K-1} \\sum_{n=1}^{N-1} \\xi(z_{n-1,j},z_{n,l})} $$\n",
        "\n",
        "$$ \\mathbf{\\mu}_k = \\frac{\\sum_{n=0}^{N-1} \\gamma(z_{n,k}) \\mathbf{x}_n}{\\sum_{n=0}^{N-1} \\gamma(z_{n,k})} $$\n",
        "\n",
        "$$ \\mathbf{\\Sigma}_k = \\frac{\\sum_{n=0}^{N-1} \\gamma(z_{n,k}) (\\mathbf{x}_n-\\mathbf{\\mu}_k) (\\mathbf{x}_n-\\mathbf{\\mu}_k)^T}{\\sum_{n=0}^{N-1} \\gamma(z_{n,k})} $$\n",
        "\n",
        "You repeat this process by replacing $ \\mathbf{\\theta}^{old} $ with this new $ \\mathbf{\\theta} $, and you will eventually get the optimal results $ \\hat{\\mathbf{\\theta}} $ to maximize (1).\n",
        "\n",
        "In practice, $ \\alpha() $ and $ \\beta() $ will quickly go to zero (because it's recursively multiplied by $ p(\\mathbf{x}_n|\\mathbf{\\mu}_k^{old}, \\mathbf{\\Sigma}_k^{old}) $ and $ A_{j,k}^{old} $) and it will then exceed the dynamic range of precision in computation, when $ N $ is large.<br>\n",
        "For this reason, the coefficients, called **scaling factors**, will be introduced to normalize $ \\alpha() $ and $ \\beta() $ in each step $ n $. The scaling factors will be canceled in EM algorithms, however, when you monitor the value of likelihood functions, you'll need to record scaling factors and apply these factors."
      ],
      "metadata": {
        "id": "SVvDakWjc-Qe"
      },
      "id": "SVvDakWjc-Qe"
    },
    {
      "cell_type": "markdown",
      "id": "4f60f500",
      "metadata": {
        "id": "4f60f500"
      },
      "source": [
        "## 0. Prerequisites"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4baaa78",
      "metadata": {
        "id": "f4baaa78"
      },
      "outputs": [],
      "source": [
        "!pip3 install numpy\n",
        "!pip3 install scipy"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa3be6ac",
      "metadata": {
        "id": "aa3be6ac"
      },
      "source": [
        "## 1. Initialize parameters\n",
        "\n",
        "First, initialize $ \\mathbf{\\theta} = \\{ \\pi_k, \\mathbf{A}_{j,k}, \\mathbf{\\mu}_k, \\mathbf{\\Sigma}_k \\} $ as follows.\n",
        "\n",
        "- $ \\pi_0 = 0.3, \\pi_1 = 0.3, \\pi_2 = 0.4 $\n",
        "- $ A_{i,j} = 0.4 $ if $ i=j $, and $ A_{i,j} = 0.3 $ otherwise\n",
        "- $ \\mathbf{\\mu}_k = (1.0, 1.0) \\;\\;\\; (k = 0,1,2) $\n",
        "- $ \\mathbf{\\Sigma}_k = \\begin{bmatrix} 1.0 & 0.5 \\\\ 0.5 & 1.0 \\end{bmatrix} \\;\\;\\; (k = 0,1,2) $\n",
        "\n",
        "In this example, we set the fixed values. However, in practice, K-means will be used to determine initial $ \\mathbf{\\mu}_k $ and $ \\mathbf{\\Sigma}_k $, in order to speed up optimization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b114a5c1",
      "metadata": {
        "id": "b114a5c1"
      },
      "outputs": [],
      "source": [
        "# Initialize parameters\n",
        "theta_old = {\n",
        "    \"pi\":[0.3, 0.3, 0.4],\n",
        "    \"A\":[[0.4,0.3,0.3],[0.3,0.4,0.3],[0.3,0.3,0.4]],\n",
        "    \"mu\":[[1.0,1.0],[1.0,1.0],[1.0,1.0]],\n",
        "    \"Sigma\":[\n",
        "        [[1.0,0.5],[0.5,1.0]],\n",
        "        [[1.0,0.5],[0.5,1.0]],\n",
        "        [[1.0,0.5],[0.5,1.0]]\n",
        "    ]\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb646728",
      "metadata": {
        "id": "bb646728"
      },
      "source": [
        "## 2. Get $ \\alpha() $ and $ \\beta() $ (3 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d8a4fc8",
      "metadata": {
        "id": "2d8a4fc8"
      },
      "source": [
        "Now we set the starting condition, $ \\alpha(z_{0,k}) $. :\n",
        "\n",
        "$$ \\alpha(z_{0,k}) = \\pi_k^{old} p(\\mathbf{x}_0|\\mathbf{\\mu}_k^{old}, \\mathbf{\\Sigma}_k^{old}) $$\n",
        "\n",
        "And we can recursively obtain all $ \\alpha(z_{n,k}) $ as follows.\n",
        "\n",
        "$$ \\alpha(z_{n,k}) = p(\\mathbf{x}_n|\\mathbf{\\mu}_k^{old}, \\mathbf{\\Sigma}_k^{old}) \\sum_{j=0}^{K-1} A_{jk}^{old} \\alpha(z_{n-1,j}) $$\n",
        "\n",
        "We also introduce a scaling factor in each step to prevent the overflow of dynamic range. In practice, the scaling factors can be shared between $ \\alpha() $ and $ \\beta() $ (and you can then use these shared values for getting values of likelihood function), but in this example, we can simply normalize values in each step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "179fc0b8",
      "metadata": {
        "id": "179fc0b8"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import multivariate_normal\n",
        "\n",
        "def get_alpha():\n",
        "    alpha = np.empty((0,3))\n",
        "\n",
        "    # Get initial alpha_0\n",
        "    alpha_0 = np.array([])\n",
        "    for k in range(3):\n",
        "        p_dist = multivariate_normal(\n",
        "            mean=theta_old[\"mu\"][k],\n",
        "            cov=theta_old[\"Sigma\"][k])\n",
        "        alpha_0 = np.append(alpha_0, theta_old[\"pi\"][k] * p_dist.pdf(X[0]))\n",
        "    alpha_0 = alpha_0 / alpha_0.sum()  # apply scaling\n",
        "    alpha = np.vstack((alpha, alpha_0))\n",
        "\n",
        "    # Get all elements recursively\n",
        "    for n in range(1, N):\n",
        "        alpha_n = np.array([])\n",
        "        for k in range(3):\n",
        "            p_dist = multivariate_normal(\n",
        "                mean=theta_old[\"mu\"][k],\n",
        "                cov=theta_old[\"Sigma\"][k])\n",
        "            alpha_n = np.append(\n",
        "                alpha_n,\n",
        "                p_dist.pdf(X[n]) * sum((theta_old[\"A\"][j][k] * alpha[n-1][j]) for j in range(3)))\n",
        "        alpha_n = alpha_n / alpha_n.sum()  # apply scaling\n",
        "        alpha = np.vstack((alpha, alpha_n))\n",
        "\n",
        "    return alpha"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a216fa5",
      "metadata": {
        "id": "7a216fa5"
      },
      "source": [
        "We also set the starting condition, $ \\beta(z_{N-1,k}) $. :\n",
        "\n",
        "$$ \\beta(z_{N-1,k}) = 1 $$\n",
        "\n",
        "And we can recursively obtain all $ \\beta(z_{n,k}) $ as follows.\n",
        "\n",
        "$$ \\beta(z_{n-1,k}) = \\sum_{j=0}^{K-1} A^{old}_{k,j} p(\\mathbf{x}_{n}|\\mathbf{\\mu}_j^{old}, \\mathbf{\\Sigma}_j^{old}) \\beta(z_{n,j}) $$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e94369b",
      "metadata": {
        "id": "6e94369b"
      },
      "outputs": [],
      "source": [
        "def get_beta():\n",
        "    beta_rev = np.empty((0,3))\n",
        "\n",
        "    # TODO: Get initial beta_{N-1}\n",
        "\n",
        "\n",
        "    # TODO: Get all elements recursively\n",
        "\n",
        "\n",
        "    # Reverse results\n",
        "    beta = np.flip(beta_rev, axis=0)\n",
        "\n",
        "    return beta"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e486bdfc",
      "metadata": {
        "id": "e486bdfc"
      },
      "source": [
        "## 3. Get $ \\gamma() $ and $ \\xi() $ (3 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee549e91",
      "metadata": {
        "id": "ee549e91"
      },
      "source": [
        "Now we obtain $ \\gamma() $ and $ \\xi() $ with previous $ \\alpha() $ and $ \\beta() $.<br>\n",
        "First we get $ \\gamma() $ as follows. (Note that the value should be normalized)\n",
        "\n",
        "$$ \\gamma(z_{n,k}) = \\frac{\\alpha(z_{n,k})\\beta(z_{n,k})}{\\sum_{k=0}^{K-1} \\alpha(z_{n,k})\\beta(z_{n,k})} $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4282314b",
      "metadata": {
        "id": "4282314b"
      },
      "outputs": [],
      "source": [
        "def get_gamma(alpha, beta):\n",
        "    gamma = np.empty((0,3))\n",
        "\n",
        "    for n in range(N):\n",
        "        gamma_n = np.array([])\n",
        "        for k in range(3):\n",
        "            gamma_n = np.append(gamma_n, alpha[n][k] * beta[n][k])\n",
        "        gamma_n = gamma_n / gamma_n.sum()\n",
        "        gamma = np.vstack((gamma, gamma_n))\n",
        "\n",
        "    return gamma"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f0da770",
      "metadata": {
        "id": "4f0da770"
      },
      "source": [
        "Next we also get $ \\xi() $ as follows. (Note that the value should be normalized)\n",
        "\n",
        "$$ \\xi(z_{n-1,j},z_{n,k}) = \\frac{\\alpha(z_{n-1,j})p(\\mathbf{x}_n|\\mathbf{\\mu}_k^{old}, \\mathbf{\\Sigma}_k^{old})A_{j,k}^{old}\\beta(z_{n,k})}{\\sum_{j=0}^{K-1} \\sum_{k=0}^{K-1} \\alpha(z_{n-1,j})p(\\mathbf{x}_n|\\mathbf{\\mu}_k^{old}, \\mathbf{\\Sigma}_k^{old})A_{j,k}^{old}\\beta(z_{n,k})} $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ec9dfd4",
      "metadata": {
        "id": "1ec9dfd4"
      },
      "outputs": [],
      "source": [
        "def get_xi(alpha, beta):\n",
        "    xi = np.empty((0,3,3))\n",
        "\n",
        "    # TODO: Compute xi\n",
        "\n",
        "    return xi"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e27e5dc",
      "metadata": {
        "id": "2e27e5dc"
      },
      "source": [
        "## 4. Get new (optimal) parameters $ \\mathbf{\\theta} $ (4 points)\n",
        "\n",
        "Finally, get new $ \\mathbf{\\theta} = \\{ \\pi_k, A, \\mathbf{\\mu}, \\mathbf{\\Sigma} \\} $ using previous $ \\gamma() $ and $ \\xi() $."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60333624",
      "metadata": {
        "id": "60333624"
      },
      "source": [
        "First, $ \\pi_k \\; (k=0,1,2) $ is given as follows. (The obtained $ \\gamma() $ is fed into the following equation.)\n",
        "\n",
        "$$ \\pi_k = \\frac{\\gamma(z_{0,k})}{\\sum_{j=0}^{K-1} \\gamma(z_{0,j})} $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0b47105",
      "metadata": {
        "id": "c0b47105"
      },
      "outputs": [],
      "source": [
        "def get_pi_new(gamma):\n",
        "    pi_new = np.array([])\n",
        "\n",
        "     # TODO: Compute pi_new\n",
        "\n",
        "    return pi_new"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44ae3cdf",
      "metadata": {
        "id": "44ae3cdf"
      },
      "source": [
        "$ A_{j,k} \\; (j,k=0,1,2) $ is given as follows.\n",
        "\n",
        "$$ A_{j,k} = \\frac{\\sum_{n=1}^{N-1} \\xi(z_{n-1,j},z_{n,k})}{\\sum_{l=0}^{K-1} \\sum_{n=1}^{N-1} \\xi(z_{n-1,j},z_{n,l})} $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a686f2e",
      "metadata": {
        "id": "8a686f2e"
      },
      "outputs": [],
      "source": [
        "def get_A_new(xi):\n",
        "    A_new = np.zeros((3,3), dtype=np.float64)\n",
        "\n",
        "     # TODO: Compute A_new\n",
        "\n",
        "    return A_new"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff6dae4a",
      "metadata": {
        "id": "ff6dae4a"
      },
      "source": [
        "$ \\mathbf{\\mu}_{k} \\; (k=0,1,2) $ is given as follows.\n",
        "\n",
        "$$ \\mathbf{\\mu}_k = \\frac{\\sum_{n=0}^{N-1} \\gamma(z_{n,k}) \\mathbf{x}_n}{\\sum_{n=0}^{N-1} \\gamma(z_{n,k})} $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98940314",
      "metadata": {
        "id": "98940314"
      },
      "outputs": [],
      "source": [
        "def get_mu_new(gamma):\n",
        "    mu_new = np.zeros((3,2), dtype=np.float64)\n",
        "\n",
        "     # TODO: Compute mu_new\n",
        "\n",
        "    return mu_new"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9fa5659c",
      "metadata": {
        "id": "9fa5659c"
      },
      "source": [
        "$ \\mathbf{\\Sigma}_{k} \\; (k=0,1,2) $ is given as follows.\n",
        "\n",
        "$$ \\mathbf{\\Sigma}_k = \\frac{\\sum_{n=0}^{N-1} \\gamma(z_{n,k}) (\\mathbf{x}_n-\\mathbf{\\mu}_k) (\\mathbf{x}_n-\\mathbf{\\mu}_k)^T}{\\sum_{n=0}^{N-1} \\gamma(z_{n,k})} $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52d9e96a",
      "metadata": {
        "id": "52d9e96a"
      },
      "outputs": [],
      "source": [
        "def get_Sigma_new(gamma, mu_new):\n",
        "    Sigma_new = np.empty((0,2,2))\n",
        "\n",
        "     # TODO: Compute Sigma_new\n",
        "\n",
        "    return Sigma_new"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7786d473",
      "metadata": {
        "id": "7786d473"
      },
      "source": [
        "## 5. Run algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "566f0062",
      "metadata": {
        "id": "566f0062"
      },
      "outputs": [],
      "source": [
        "for loop in range(100):\n",
        "    print(\"Running iteration {} ...\".format(loop + 1), end=\"\\r\")\n",
        "    # Get alpha and beta\n",
        "    alpha = get_alpha()\n",
        "    beta = get_beta()\n",
        "    # Get gamma and xi\n",
        "    gamma = get_gamma(alpha, beta)\n",
        "    xi = get_xi(alpha, beta)\n",
        "    # Get optimized new parameters\n",
        "    pi_new = get_pi_new(gamma)\n",
        "    A_new = get_A_new(xi)\n",
        "    mu_new = get_mu_new(gamma)\n",
        "    Sigma_new = get_Sigma_new(gamma, mu_new)\n",
        "    # Replace theta and repeat\n",
        "    theta_old[\"pi\"] = pi_new\n",
        "    theta_old[\"A\"] = A_new\n",
        "    theta_old[\"mu\"] = mu_new\n",
        "    theta_old[\"Sigma\"] = Sigma_new\n",
        "\n",
        "print(\"\\nDone\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e725701f",
      "metadata": {
        "id": "e725701f"
      },
      "outputs": [],
      "source": [
        "np.set_printoptions(suppress=True)\n",
        "print(\"A\")\n",
        "print(A_new)\n",
        "print(\"Mu\")\n",
        "print(mu_new)\n",
        "print(\"Sigma\")\n",
        "print(Sigma_new)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}